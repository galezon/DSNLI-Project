---
title: "Risk Loading"
output: html_notebook
---

Load all models, functions and libraries.
```{r}
freq_glm = readRDS("../models/freq_glm_poisson.Rda")
sev_glm = readRDS("../models/sev_glm_gamma.Rda")

source("../r_utils/load_data_post_binning.R")

library(dplyr)
library(robustbase)
```

Load datasets.
```{r}
# bootstrap sampling below explicitly requires full_dataset to be defined
full_dataset = load_data_post_binning("../initial_docs/Assignment.csv")

# set the NAs automatically to zero afterwards, we do want 0/0 = 0 in this case
full_dataset["avg_claim"] = full_dataset["chargtot"] / full_dataset["nbrtotc"]
full_dataset[is.na(full_dataset["avg_claim"]), "avg_claim"] = 0
```

What I will implement is a simple $\Pi(X) = (1+\theta)\mathbb{E}X$, where $X$ is the risk characteristic ($X= \texttt{Freq}\cdot\texttt{Sev}$). I couldn't find an exact idea on what should $\theta$ be, merely that it is positive. I have seen ideas here and there that $\theta$ should be such that the probability of ruin (having less cash reserves than claims) is small, say $\varepsilon$.

So, what I will do is build a bootstrap algorithm to the GLMs we already have. We keep the GLMs for frequency and severity we already have to create tariffs, we instead create say $B=100$ bootstrapped portfolios. For each portfolio $P_i$, we have the total tariffs $T_i$ that we predicted with our GLMs, and the total number of claims ( sum(chargtot) ) $CT_i$. We then set $\theta$ as the number such that at least $(1-\varepsilon)B$ portfolios have $(1+\theta)T_i \geq CT_i$.

Finding the tariff for an individual is simple enough, we simply have to run our two glm models and take the product. Finding the *chargtot* for an individual is more complicated however:

* **nbrtotc**: When calculating the tariff we had to find mean of the frequency $\hat{F}_i$. Since we used a Poisson GLM, we can simply sample from a $Poi(\hat{F}_i)$ and set that as the bootstrapped *nbrtotc*.
* **avg_claim**: Since our GLM only contains categorical variables, what I will do is gather all other individuals within the same risk category. This will allows us to have an observation set of *avg_claim* for each risk category, and I will *fit* in a Gamma distribution for each risk category. The two parameters for the gamma distribution will be estimated via method of moments. We then sample from the gamma distribution to get the *avg_claim* for a single bootstrap observation.

The bootstrap procedure for *avg_claim* is already a simplification, I think there is another way to properly bootstrap a Gamma glm, via shape-mean parametrization, but I do not have the necessary knowledge at hand at the moment.

I will keep the functions I need here in this file, so that it's less of a hassle to check what I did.


Function below generates a single bootstrap dataset as described above.
```{r}
generate_single_bootstrap_dataset = function(full_dataset, frac_observations,
                                             freq_glm, sev_glm) {

  tmp1 = grab_bootstrap_sample(full_dataset, frac_observations)  # grab observations first
  tmp2 = add_predictions(tmp1, freq_glm, sev_glm)
  tmp3 = add_bootstrap_nbrtotc(tmp2)
  tmp4 = add_bootstrap_avgclaim_and_chargtot(full_dataset, tmp3)
  
  return(tmp4)
}

```


I will generate 50 datasets with 30% number of samples each, and we can check the dataset properties. I will also throw away most of the columns, only keeping necessary ones.
```{r}
dataset_list = list()
# for (idx in 1:50) {
#   tmp_bootstrap_sample = generate_single_bootstrap_dataset(full_dataset, 0.3,
#                                                            freq_glm, sev_glm)
#   to_store = tmp_bootstrap_sample %>% select(nbrtotc, avg_claim, chargtot,
#                                             bootstrap_nbrtotc, bootstrap_avg_claim,
#                                              bootstrap_chargtot)
#   dataset_list[[idx]] = to_store
# }
```

Save bootstrap since it took so long.
```{r}
# saveRDS(dataset_list, "../data/bootstrapped_data.Rda")
```

The bootstrap results. Some sanity metrics for the bootstrap results:

* In each portfolio, gather $\frac{\sum_i(boot\_chargtot)_i}{\sum_i(chargtot)_i}$.

* Similarly $\frac{\sum_i(boot\_nbrtotc)_i}{\sum_i(nbrtotc)_i}$.

We then plot these points and see that they are scattered around 1, that is good, since that is a rough indicator of behaving like the original portfolio. Their variance is also rather small, further indicating that we didn't do that bad of a job in the bootstrap.

```{r}
s_chargtot = rep(0, 50)
s_nbrtotc = rep(0, 50)
b_s_chargtot = rep(0, 50)
b_s_nbrtotc = rep(0, 50)
for (idx in 1:50) {
  tmp_data = dataset_list[[idx]]
  s_chargtot[idx] = sum(tmp_data$chargtot)
  s_nbrtotc[idx] = sum(tmp_data$nbrtotc)
  b_s_chargtot[idx] = sum(tmp_data$bootstrap_chargtot)
  b_s_nbrtotc[idx] = sum(tmp_data$bootstrap_nbrtotc)
}
par(mfrow=c(1, 2))
plot(b_s_chargtot/s_chargtot, main="sum(boot_chargtot) / sum(chargtot)")
plot(b_s_nbrtotc/s_nbrtotc, main="sum(boot_nbrtotc) / sum(nbrtotc)")

print(paste("Mean of ratio of sum(bootstrap_chargtot)/sum(chargtot) = ",
            mean(b_s_chargtot/s_chargtot), sep=""))
print(paste("Variance of ratio of sum(bootstrap_chargtot)/sum(chargtot) = ",
            var(b_s_chargtot/s_chargtot), sep=""))

print(paste("Mean of ratio of sum(bootstrap_nbrtotc)/sum(nbrtotc) = ",
            mean(b_s_nbrtotc/s_nbrtotc), sep=""))
print(paste("Variance of ratio of sum(bootstrap_nbrtotc)/sum(nbrtotc) = ",
            var(b_s_nbrtotc/s_nbrtotc), sep=""))
```

Now, we need to gather the predicted tariffs as well. I accidentally dropped the tariffs but luckily each dataset still has the indexes.
```{r}
get_tariffs_sum = function(full_dataset, boot_dataset){
  indexes = as.numeric(rownames(boot_dataset))
  tmp_data = add_predictions(full_dataset[indexes,], freq_glm, sev_glm)
  
  return(sum(tmp_data[,"tariff_preds"]))
}

s_tariffs = rep(0, 50)
for (idx in 1:50) {
  s_tariffs[idx] = get_tariffs_sum(full_dataset, dataset_list[[idx]])
}
```

Now we can finally find an appropriate value for $\theta$. In $42\%$ of cases, the total tariffs is lower than the total (bootstrapped) claims.
```{r}
sum_ratios = (s_tariffs - b_s_chargtot)/b_s_chargtot
plot(sum_ratios, main="(s_tariffs - b_s_chargtot)/b_s_chargtot")
abline(h=0)
print(paste("Percentage of datasets with total tariffs lower than total claims =",1 - sum(sum_ratios>= 0)/length(sum_ratios)))
```

What we would like to do is to find $\theta$ such that $(1 + \theta) \sum \texttt{tariffs} \geq \sum \texttt{chargtot}$ with probability at least $1 - \varepsilon$. We have data $\{\sum\texttt{tariff}, \sum\texttt{chargtot} \}_i$.

Suppose we want $(1 + \theta)a \geq b$, then the solution is $\theta = \frac{b}{a} - 1$. Thus, we can simply order $\{\frac{\sum\texttt{tariffs}}{\sum\texttt{chargtot}} - 1\}$, take the an appropriae quantile, and set that as $\theta$.

For example, if we want at least $95\%$ of the time we have a higher tariff compared to total claims, then we can set $\theta = 0.27$ here.
```{r}
sort(b_s_chargtot/s_tariffs) - 1
```

```{r}
sum_ratios2 = (s_tariffs*1.27 - b_s_chargtot)/b_s_chargtot
plot(sum_ratios2, main="(s_tariffs_risk_loaded - b_s_chargtot)/b_s_chargtot")
abline(h=0)
print(paste("Percentage of datasets with total tariffs lower than total claims =",1 - sum(sum_ratios2>= 0)/length(sum_ratios)))
```


# Helper functions to create a single bootstrap sample

```{r}
grab_bootstrap_sample = function(data, boot_sample_size) {
  size = floor(boot_sample_size * nrow(data))
  indexes = sample(nrow(data), size=size, replace=TRUE)
  
  return(data[indexes, ])
}
```

```{r}
add_predictions = function(bootstrap_data, freq_glm, sev_glm) {
  tmp_data = bootstrap_data  # never mutate incoming data!
  
  tmp_data["freq_preds"] = predict.glm(object=freq_glm,
                                       newdata=tmp_data,
                                       type="response")
  tmp_data["sev_preds"] = predict.glm(object=sev_glm,
                                      newdata=tmp_data,
                                      type="response")
  tmp_data["tariff_preds"] = tmp_data["freq_preds"] * tmp_data["sev_preds"]
  
  return(tmp_data)
}
```

```{r}
add_bootstrap_nbrtotc = function(bootstrap_data_with_predictions) {
  tmp_data = bootstrap_data_with_predictions

  get_pois = function(row) {
    return(rpois(1, as.numeric(row['freq_preds'])))
  }
  
  tmp_data["bootstrap_nbrtotc"] = apply(tmp_data, 1, get_pois)
  
  return(tmp_data)
}
```

```{r}
add_bootstrap_avgclaim_and_chargtot = function(full_dataset, bootdata_w_preds_and_boot_nbrtotc) {
  tmp_data = bootdata_w_preds_and_boot_nbrtotc
  
  tmp_data["bootstrap_avg_claim"] = apply(tmp_data, 1, get_gamma)
  tmp_data["bootstrap_chargtot"] = tmp_data["bootstrap_avg_claim"] * tmp_data["bootstrap_nbrtotc"]
  
  return(tmp_data) 
  
}

get_gamma = function(row) {
  tmp_fuelc = getElement(row, "fuelc")
  tmp_split = getElement(row, "split")
  tmp_sev_AGEPH = getElement(row, "sev_AGEPH")
  tmp_sev_codposs = getElement(row, "sev_codposs")
  
  # get common risk class, and condition only on conditions where avg_claim is larger than 0
  risk_class = full_dataset %>% filter(fuelc==tmp_fuelc, split==tmp_split,
                                       sev_AGEPH==tmp_sev_AGEPH,
                                       sev_codposs==tmp_sev_codposs, avg_claim > 0)
  avg_claims = risk_class$avg_claim
  
  if(nrow(risk_class) == 0) {  # no valid observations => just return global mean
#    full_dataset %>% filter(avg_claim > 0) %>% select(avg_claim) %>% summarize(mean = 
#                            mean(avg_claim))
#     mean
# 1 1627.996
    return(1628)
  }
  else if (nrow(risk_class) <= 3) {  # too low to try anything fancy, just return the mean
    return( round(mean(avg_claims), 0) ) 
  } 
  else {
    # method of moments
    mu = mean(avg_claims)
    s = sd(avg_claims)
    
    k = (mu/s)^2
    theta = (s^2)/mu
    
    return(round(rgamma(n=1, shape=k, scale=theta), 0))
  }
}

```





























































